{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tx.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN/RzXDzxd4m1APoM56uvcW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oydpEwBk9Ott","colab_type":"code","colab":{}},"source":["import time\n","import numpy as np\n","import tensorflow as tf\n","import transformer # import library rectified using reference - https://www.tensorflow.org/tutorials/text/transformer\n","from sklearn.metrics import mean_absolute_error as mae\n","\n","# Transformer - class object\n","# Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, kernel_size, rate=0.1)\n","  # num_layers = number of encoder & decoder layers\n","  # d_model = number of features to be created from input for multihead-attention\n","  # num_heads = number of heads (depth = d_model / num_heads)\n","  # dff = hidden neuron number of feed forward network\n","  # input_vocab_size = 1 (there is only 1 feature)\n","  # target_vocab_size = 1 (there is only 1 feature)\n","  # pe_input = maximum position encoding length (not affect result as long as it is longer than the sequence)\n","  # pe_target = maximum position encoding length (not affect result as long as it is longer than the sequence)\n","  # kernel_size = convolution kernel size (NLP transformer embedding layer is replaced with convolution layer)\n","  # rate = dropout rate\n","\n","# create_padding_mask - function (no needed in this work as there is no padding - all input are of the same length)\n","# create_look_ahead_mask - function (needed for decoder to mask the future target value to prevent information leak)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"em8f1EuZ9a5-","colab_type":"code","colab":{}},"source":["class tx_model():\n","  def __init__(self, num_layers, d_model, num_heads, dff,\n","               kernel_size, dropout_rate, timesteps_in, \n","               timesteps_out, item, directory):\n","\n","    self.timesteps_in = timesteps_in\n","    self.timesteps_out = timesteps_out\n","\n","    # fixed transformer parameters\n","    input_vocab_size = 1 # there is only 1 feature\n","    target_vocab_size = 1 # there is only 1 feature\n","    pe_input = 10 * timesteps_in\n","    pe_target = 10 * timesteps_out\n","\n","    # create transformer model\n","    self.model = transformer.Transformer(num_layers, d_model, num_heads, dff,\n","                                         input_vocab_size, target_vocab_size, \n","                                         pe_input, pe_target, kernel_size,\n","                                         dropout_rate)\n","\n","    # training parameters\n","    self.weights_dir = directory + \"TX_\" + str(item) + \"_k\" + str(kernel_size) + \\\n","                       \"_dm\" + str(d_model) + \"_df\" + str(dff) + \"_l\" + str(num_layers) + \\\n","                       \"_h\" + str(num_heads) + \"_weights.h5py\"\n","    self.epochs = 30\n","    self.threshold = 0.005\n","    self.batch_size = 64\n","    self.optimizer = tf.keras.optimizers.Adam(0.0001)\n","    self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n","\n","  # loss function for training\n","  def loss_function(self, real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = tf.expand_dims(tf.keras.losses.MAE(real, pred), axis=-1)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","\n","  # create padding msks for encoder and decoder & look ahead mask for decoder\n","  def create_masks(self, inp, tar):\n","\n","    # Create look ahead mask - location of future token will be 1, \n","    # Since there is no need for padding mask, the output will all be zero\n","    # Combine the look ahead and padding mask\n","    # For NLP transformer, the input is (x,y)\n","    # For modified time series transformer, the input is (x,y,1)\n","    # Thus the dimension is different when creating padding and modification is required\n","\n","    # Encoder padding mask\n","    enc_padding_mask = transformer.create_padding_mask(inp)\n","    enc_padding_mask = enc_padding_mask[:,:,:,:,0] # ensure consistent dimension\n","    \n","    # Used in the 2nd attention block in the decoder.\n","    # This padding mask is used to mask the encoder outputs.\n","    dec_padding_mask = transformer.create_padding_mask(inp)\n","    dec_padding_mask = dec_padding_mask[:,:,:,:,0] # ensure consistent dimension\n","    \n","    # Used in the 1st attention block in the decoder.\n","    # It is used to pad and mask future tokens in the input received by the decoder.\n","    look_ahead_mask = transformer.create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = transformer.create_padding_mask(tar)\n","    dec_target_padding_mask = dec_target_padding_mask[:,:,:,:,0] # ensure consistent dimension\n","\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","    return enc_padding_mask, combined_mask, dec_padding_mask\n","\n","  # fit the transformer model\n","  def fit_model(self, x_train, y_train, x_valid, y_valid):\n","\n","    def get_batch(inp, tar, batch_size=1):\n","        l = len(inp)\n","        for i in range(0, l, batch_size):\n","            yield i / batch_size, inp[i:min(i + batch_size, l)], tar[i:min(i + batch_size, l)]\n","\n","    # change to the default dtype of tensorflow layer\n","    x_train = tf.cast(x_train, dtype=tf.float32)\n","    y_train = tf.cast(y_train, dtype=tf.float32)\n","\n","    val_loss_bef = np.inf\n","    for epoch in range(self.epochs):\n","      start = time.time()\n","      train_loss = 0\n","\n","      # batch training\n","      for batch, inp, tar in get_batch(x_train, y_train, batch_size=self.batch_size):\n","          inp = inp[:, :, :]\n","          tar_inp = inp[:, -self.timesteps_out:, :]\n","          tar_real = tar[:, :, 0:1]  \n","          \n","          # enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(inp, tar_inp)\n","          enc_padding_mask, combined_mask, dec_padding_mask = None, None, None\n","          \n","          with tf.GradientTape() as tape:\n","            predictions, _ = self.model(inp, tar_inp, \n","                                        True,\n","                                        enc_padding_mask, \n","                                        combined_mask, \n","                                        dec_padding_mask)\n","            loss = self.loss_function(tar_real, predictions) # Find the loss between target and prediction\n","\n","          gradients = tape.gradient(loss, self.model.trainable_variables) # Compute the first derivative of the weights\n","          self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # Backpropagation \n","          train_loss += loss\n","\n","      # Save the weights of best model\n","      y_valid_pred, _, _ = self.predict(x_valid, True)\n","      val_loss = mae(y_valid[:, :, 0], y_valid_pred[:, :, 0])\n","      \n","      if val_loss < val_loss_bef:\n","        self.model.save_weights(self.weights_dir)\n","        val_loss_bef = val_loss\n","\n","      print('Epoch {} training loss - {:.4f}, validation loss - {:.4f}'.format(epoch + 1, train_loss / (batch + 1), val_loss))\n","      print('Best validation loss - {:.4f}, time taken for 1 epoch {:.4f} sec\\n'.format(val_loss_bef, time.time() - start))\n","\n","  # make prediction\n","  def predict(self, x_test, training):\n","\n","    def get_batch(inp, batch_size=1):\n","        l = len(inp)\n","        for i in range(0, l, batch_size):\n","            yield i / batch_size, inp[i:min(i + batch_size, l)]\n","    \n","    # only load the save weights only if the model is being trained\n","    if not training:\n","      self.model.load_weights(self.weights_dir)\n","\n","    # change to the default dtype of tensorflow layer\n","    x_test = tf.cast(x_test, dtype=tf.float32)\n","\n","    for batch, inp in get_batch(x_test, batch_size=self.batch_size):\n","      # set the last t step of input as the decoder input to make prediction\n","      encoder_input = inp[:, :, :]\n","      decoder_input = inp[:, -self.timesteps_out:, :]\n","\n","      # enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(encoder_input, decoder_input)\n","      enc_padding_mask, combined_mask, dec_padding_mask = None, None, None\n","\n","      # predictions.shape == (batch_size, seq_len, vocab_size)\n","      # set dropout to be false\n","      predictions, attention_weights = self.model(encoder_input, \n","                                                  decoder_input,\n","                                                  False, \n","                                                  enc_padding_mask,\n","                                                  combined_mask,\n","                                                  dec_padding_mask)\n","\n","      if batch == 0:\n","        y_test_pred = predictions\n","        #attentions = attention_weights\n","      else:\n","        y_test_pred = tf.concat([y_test_pred, predictions], axis=0)\n","        #attentions = tf.concat([attentions, attention_weights], axis=0)\n","\n","    # attention_weights are saved for the last batch only\n","    # attention_weights contain the attention of decoder input to encoder output & decoder input self attention\n","    return y_test_pred, attention_weights, decoder_input"],"execution_count":0,"outputs":[]}]}