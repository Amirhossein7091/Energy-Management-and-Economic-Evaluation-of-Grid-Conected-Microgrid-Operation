{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seqatt.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMeWmYYM260CT+uDejhjiTI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RODX1pEa3VGw","colab_type":"code","colab":{}},"source":["import h5py\n","import numpy as np\n","import tensorflow as tf\n","tf.keras.backend.set_floatx('float32')\n","\n","class Encoder(tf.keras.Model):\n","  def __init__(self, encoder_units, conv1d_units, batch_size):\n","    super(Encoder, self).__init__()\n","    self.batch_size = batch_size\n","    self.encoder_units = encoder_units\n","    self.weights_dir = \"/content/drive/My Drive/Colab/Research - EE5003/Phase 1 - Comparison of forecasting technique/Model Weights/encoder.h5py\"\n","    self.lstm = tf.keras.layers.LSTM(encoder_units,\n","                                     return_sequences=True,\n","                                     return_state=True)\n","    \n","  def call(self, x, state_h, state_c):\n","    # output - (batch_size, timesteps_in, encoder_units)\n","    # state_h, state_c - (batch_size, encoder_units)\n","    output, state_h, state_c = self.lstm(x, initial_state=[state_h, state_c])\n","    return output, state_h, state_c\n","  \n","  def initialize_state(self):\n","    state_h = tf.zeros((self.batch_size, self.encoder_units))\n","    state_c = tf.zeros((self.batch_size, self.encoder_units))\n","    return state_h, state_c\n","\n","class Decoder(tf.keras.Model):\n","  def __init__(self, decoder_units, conv1d_units):\n","    super(Decoder, self).__init__()\n","    self.weights_dir = \"/content/drive/My Drive/Colab/Research - EE5003/Phase 1 - Comparison of forecasting technique/Model Weights/decoder.h5py\"\n","    self.conv1d_s = tf.keras.layers.Conv1D(conv1d_units, kernel_size=1, \n","                                           strides=1, padding=\"same\", \n","                                           activation=\"relu\")\n","    self.conv1d_h = tf.keras.layers.Conv1D(conv1d_units, kernel_size=1, \n","                                           strides=1, padding=\"same\", \n","                                           activation=\"relu\")\n","    self.conv1d_v = tf.keras.layers.Conv1D(1, kernel_size=1, \n","                                           strides=1, padding=\"same\", \n","                                           activation=\"sigmoid\")     \n","    self.conv1d_output = tf.keras.layers.Conv1D(1, kernel_size=1, \n","                                                strides=1, padding=\"same\", \n","                                                activation=\"linear\")    \n","    self.lstm = tf.keras.layers.LSTM(decoder_units,\n","                                     return_sequences=True,\n","                                     return_state=True)\n","    self.tanh = tf.keras.layers.Activation(\"tanh\")\n","    self.softmax = tf.keras.layers.Softmax(axis=1)\n","\n","  def call(self, dec_input, enc_output, state_h, state_c):\n","    \"\"\"\n","    1. Use the hidden and cell state from encoder's last state as the initial \n","    state of decoder lstm\n","    2. score(t) = w_v * (tanh(w_s * enc_output + w_h * state_h(t)))\n","    3. attention(t_i) = exp(score(t_i)) / sum(exp(score(t_i)))\n","    4. content_vector = sum(attention * enc_output) - sum across all timesteps\n","    \"\"\"\n","\n","    # dec_hidden - (batch_size, encoder_units)\n","    dec_hidden = [state_h, state_c]\n","\n","    # expand state_h to match enc_output dimension\n","    # state_h_time, state_c_time - (batch_size, 1, encoder_units)\n","    # state_hc_time - (batch_size, 1, 2 * encoder_units)\n","    state_h_time = tf.expand_dims(state_h, axis=1)\n","    state_c_time = tf.expand_dims(state_c, axis=1)\n","    state_hc_time = tf.concat([state_h_time, state_c_time], axis=-1)\n","\n","    # additive score\n","    # score_s - (batch_size, timesteps_in, conv1d_units)\n","    # score_hc - (batch_size, 1, conv1d_units)\n","    # score - (batch_size, timesteps_in, 1)\n","    score_s = self.conv1d_s(enc_output)\n","    score_h = self.conv1d_h(state_hc_time)\n","    score = self.tanh(score_s + score_h)\n","    score = self.conv1d_v(score)\n","\n","    # attention weights - (batch_size, timesteps_in, 1)\n","    attention_weights = self.softmax(score)\n","  \n","    # content vector - before sum (batch_size, timesteps_in, encoder_units)\n","    # content vector - after sum (batch_size, encoder_units)\n","    # content vector - after expansion (batch_size, 1, encoder_units)\n","    # content vector - after concatenation (batch_size, 1, encoder_units + 1)\n","    content_vector = attention_weights * enc_output\n","    content_vector = tf.reduce_sum(content_vector, axis=1)\n","    content_vector = tf.expand_dims(content_vector, axis=1)\n","    content_vector = tf.concat([content_vector, dec_input], axis=-1) \n","    \n","    # compute the output and hidden and cell state\n","    # dec_output - (batch_size, 1, decoder_units)\n","    # state_h, state_c - (batch_size, encoder_units)\n","    dec_output, state_h, state_c = self.lstm(content_vector, initial_state=dec_hidden)\n","\n","    # output - (batch_size, 1, 1) \n","    output = self.conv1d_output(dec_output)\n","\n","    return output, attention_weights, state_h, state_c"],"execution_count":0,"outputs":[]}]}